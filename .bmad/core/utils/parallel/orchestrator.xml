<?xml version="1.0" encoding="UTF-8"?>
<!--
  PARALLEL ORCHESTRATOR UTILITY

  Purpose: Spawn and coordinate parallel terminal agents for distributed work

  Usage:
    <include href="orchestrator.xml" />
    <call-template name="spawn-parallel-agents">
      <with-param name="work_units" select="$work_list" />
      <with-param name="agent_type" select="'epic-worker'" />
      <with-param name="coordination_file" select="'.system/parallel-coordination.yaml'" />
    </call-template>

  Capabilities:
  - Spawns multiple terminal agents in parallel
  - Tracks agent progress via coordination file
  - Implements file locking with retry logic
  - Monitors terminal health and handles failures
  - Waits for all terminals to complete
  - Aggregates results from all workers
-->

<orchestrator>

  <!-- MAIN TEMPLATE: Spawn parallel agents and coordinate work -->
  <template name="spawn-parallel-agents">
    <param name="work_units" />         <!-- List of work items to distribute -->
    <param name="agent_type" />         <!-- Type of agent to spawn (epic-worker, story-worker, context-worker) -->
    <param name="coordination_file" />  <!-- Path to coordination YAML file -->
    <param name="max_terminals" default="7" /> <!-- Maximum parallel terminals -->
    <param name="context_data" />       <!-- Shared context for all agents -->

    <step n="1" goal="Initialize coordination file">
      <action>Initialize parallel coordination state</action>
      <details>
        Create or reset coordination file at {{coordination_file}}:

        ```yaml
        orchestrator:
          started_at: {{timestamp}}
          status: initializing
          total_work_units: {{count(work_units)}}
          completed: 0
          failed: 0

        terminals: {}
          # Structure:
          # terminal-1:
          #   agent_type: epic-worker
          #   work_unit: epic-2
          #   status: assigned|in-progress|completed|failed
          #   started_at: timestamp
          #   completed_at: timestamp
          #   result_file: path/to/result.md
          #   error: error message (if failed)

        work_queue:
          assigned: []
          pending: [list of work unit IDs]
          completed: []
          failed: []
        ```

        Populate work_queue.pending with all work_units IDs.
        Set orchestrator.status to "ready"
      </details>
    </step>

    <step n="2" goal="Distribute work to parallel terminals">
      <action>Spawn terminal agents with assigned work</action>
      <details>
        For each work_unit (up to max_terminals at a time):

        1. **Acquire coordination file lock**:
           - Create {{coordination_file}}.lock with retry (max 10 attempts, 500ms backoff)
           - If lock acquisition fails after retries, report error and abort

        2. **Update coordination file**:
           - Assign work_unit to terminal-{n}
           - Move work_unit from work_queue.pending to work_queue.assigned
           - Set terminal status to "assigned"
           - Record timestamp

        3. **Release lock**:
           - Delete {{coordination_file}}.lock

        4. **Spawn terminal agent**:
           Use Task tool to launch agent in new subprocess:
           ```
           Task tool:
             subagent_type: {{agent_type}}
             description: "{{agent_type}} for {{work_unit.id}}"
             prompt: |
               You are a {{agent_type}} processing {{work_unit.id}}.

               COORDINATION FILE: {{coordination_file}}
               YOUR TERMINAL ID: terminal-{n}

               WORK ASSIGNMENT:
               {{work_unit.details}}

               SHARED CONTEXT:
               {{context_data}}

               INSTRUCTIONS:
               1. Update your terminal status in coordination file:
                  - Lock file, update status to "in-progress", release lock
               2. Complete your assigned work
               3. Write output to: {{work_unit.output_file}}
               4. Update coordination file when complete:
                  - Lock file
                  - Set status to "completed"
                  - Record result_file path
                  - Move work_unit from assigned to completed
                  - Increment orchestrator.completed counter
                  - Release lock
               5. If errors occur:
                  - Lock file
                  - Set status to "failed"
                  - Record error message
                  - Move work_unit from assigned to failed
                  - Increment orchestrator.failed counter
                  - Release lock

               FILE LOCKING PROTOCOL:
               - Lock: Create {{coordination_file}}.lock
               - Retry: Up to 10 attempts with 500ms exponential backoff
               - Unlock: Delete {{coordination_file}}.lock
               - ALWAYS unlock even if operation fails
           ```

        5. **Rate limiting**:
           - If max_terminals reached, wait for one to complete before spawning next
           - Check coordination file every 5 seconds for status updates
      </details>
    </step>

    <step n="3" goal="Monitor terminal progress">
      <action>Track agent execution and report progress</action>
      <details>
        Poll coordination file every 5 seconds:

        1. **Read coordination state** (with lock):
           - Count completed, failed, in-progress terminals
           - Check for stuck terminals (in-progress > 10 minutes)

        2. **Display progress**:
           ```
           Parallel Execution Progress:
           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
           Completed: {completed}/{total} [{percentage}%]
           In Progress: {in_progress}
           Failed: {failed}
           Pending: {pending}

           Terminals:
           ✓ terminal-1: epic-2 (completed in 3m 24s)
           ⟳ terminal-2: epic-3 (in progress - 1m 45s)
           ✓ terminal-3: epic-4 (completed in 4m 12s)
           ⧗ terminal-4: epic-5 (assigned - starting...)
           ```

        3. **Handle stuck terminals**:
           - If terminal in-progress > 10min, prompt operator:
             "Terminal-2 processing epic-3 has been running for 12 minutes.
              Continue waiting or mark as failed?"

        4. **Continue until all work complete or operator aborts**
      </details>
    </step>

    <step n="4" goal="Handle completion and failures">
      <action>Process results and report final status</action>
      <details>
        When all work_units processed:

        1. **Final status report**:
           ```
           Parallel Execution Complete!
           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
           Total Time: {duration}
           Completed: {completed}/{total}
           Failed: {failed}/{total}
           Average Time per Unit: {avg_time}

           Results written to:
           {list of result files}
           ```

        2. **If failures occurred**:
           ```
           ⚠ {failed} work units failed:
           - epic-3: Error parsing PRD context (terminal-2)
           - epic-7: Timeout waiting for dependencies (terminal-5)

           Retry failed units? (y/n)
           ```

        3. **Return results**:
           - Set {{results}} variable with list of result file paths
           - Set {{failed_units}} variable with failed work unit IDs
           - Set {{execution_time}} variable with total duration
      </details>
    </step>

  </template>


  <!-- HELPER TEMPLATE: File locking with retry -->
  <template name="acquire-lock">
    <param name="lock_file" />
    <param name="max_attempts" default="10" />
    <param name="backoff_ms" default="500" />

    <logic>
      Attempt to create lock_file.
      If file exists, wait backoff_ms and retry.
      Double backoff each attempt (exponential backoff).
      After max_attempts, throw error.
      Return success when lock acquired.
    </logic>
  </template>


  <!-- HELPER TEMPLATE: Safe file unlock -->
  <template name="release-lock">
    <param name="lock_file" />

    <logic>
      Delete lock_file if it exists.
      Ignore errors (lock may have been cleaned up).
      Always return success.
    </logic>
  </template>


  <!-- HELPER TEMPLATE: Update coordination file safely -->
  <template name="update-coordination">
    <param name="coordination_file" />
    <param name="updates" /> <!-- YAML path and value pairs -->

    <logic>
      1. Acquire lock: {{coordination_file}}.lock
      2. Read current coordination file
      3. Apply updates to YAML structure
      4. Write updated coordination file
      5. Release lock
      6. If any step fails, release lock and throw error
    </logic>
  </template>

</orchestrator>
